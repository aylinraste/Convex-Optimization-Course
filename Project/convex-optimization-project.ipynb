{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Convex Optimization Project\n","\n","**Pouya Lahabi 400109834**\n","\n","**Aylin Rasteh 400104964**\n","\n","In this notebook we implemented some of the optimizations in transfer learning paper and used some of the ideas from [Multi-Task Feature Learning paper](https://proceedings.neurips.cc/paper_files/paper/2006/file/0afa92fc0f8a9cf051bf2961b06ac56b-Paper.pdf) (one of the main paper refrences)."]},{"cell_type":"markdown","metadata":{},"source":["## Abstract"]},{"cell_type":"markdown","metadata":{},"source":["A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this project we will implement parts 3.2.1 and 3.3 of the original [paper](https://www-edlab.cs.umass.edu/cs689/reading/transfer-learning.pdf)."]},{"cell_type":"markdown","metadata":{},"source":["## Notation\n","\n","We begin by introducing our notation. We let $\\mathbb{R}$ be the set of real numbers and $\\mathbb{R}^+$ ($\\mathbb{R}^{++}$) the subset of non-negative (positive) ones. Let $T$ be the number of tasks and define $\\mathbb{R}_T := \\{1, \\ldots, T\\}$. For each task $t \\in \\mathbb{R}_T$, we are given $m$ input/output examples $(x_{t1}, y_{t1}), \\ldots, (x_{tm}, y_{tm}) \\in \\mathbb{R}^d \\times \\mathbb{R}$. Based on this data, we wish to estimate $T$ functions $f_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, $t \\in \\mathbb{R}_T$, which approximate well the data and are statistically predictive.\n","\n","Let $a_i \\in \\mathbb{R}^T$ and $a_j \\in \\mathbb{R}^d$ be the $i$-th row and the $j$-th column of $A$ respectively. For every $r, p \\geq 1$, we define\n","$\n","\\|A\\|_{r,p} := \\left( \\sum_{i=1}^{d} \\|a_i\\|_p^r \\right)^{\\frac{1}{p}}.\n","$\n","\n","If $w, u \\in \\mathbb{R}^d$, we define $\\langle w, u \\rangle := \\sum_{i=1}^{d} w_i u_i$, the standard inner product in $\\mathbb{R}^d$. For every $p \\geq 1$, we define the $p$-norm of vector $w$ as $\\|w\\|_p := \\left( \\sum_{i=1}^{d} |w_i|^p \\right)^{\\frac{1}{p}}$. If $A$ is a $d \\times T$ matrix, we denote by\n","$\n","\\|A\\|_{r,p} := \\left( \\sum_{i=1}^{d} \\|a_i\\|_p^r \\right)^{\\frac{1}{p}}.\n","$\n","\n","We denote by $S_d$ the set of $d \\times d$ real symmetric matrices and by $S_d^+$ the subset of positive semidefinite ones. If $D$ is a $d \\times d$ matrix, we define $\\text{trace}(D) := \\sum_{i=1}^{d} D_{ii}$. If $X$ is a $p \\times q$ real matrix, $\\text{range}(X)$ denotes the set $\\{ x \\in \\mathbb{R}^p : x = Xz, \\text{ for some } z \\in \\mathbb{R}^q \\}$. We let $O_d$ be the set of $d \\times d$ orthogonal matrices. Finally, $D^+$ denotes the pseudoinverse of a matrix $D$."]},{"cell_type":"markdown","metadata":{},"source":["## 3.2.1 Supervised Feature Construction\n","In the inductive transfer learning setting, the common features can be learned by solving an optimization problem, given as:\n","\\begin{align*}\n","\\underset{A, U}{arg\\;min}& \\quad \\sum \\limits_{t \\in \\{T, S\\}} \\sum \\limits_{i=1}^{n_t}L\\left(y_{t_i}, \\langle a_t, U^T x_{t_i} \\rangle\\right) + \\gamma ||A||^2_{2,1} \\\\\n","s.t.& \\quad U \\in \\mathbb{O}^d\n","\\end{align*}\n","\n","This method learns a low-dimensional representation which is shared across the task.\n","\n","Solving the given task can be difficult because of two main reasons:\n","1) Although the problem is convex in each of the variables $A$ and $U$ but the main problem is non-convex.\n","2) $||A||_{2,1}$ is nonsmooth which makes it more difficult to opimize\n","\n","To this end, for every $W \\in \\mathbb{R}^{d\\times T}$ and $D \\in \\mathbb{S}^d_+$, we define the function\n","\n","$$\n","R(W, D) = \\sum\\limits_{t=1}^T \\sum\\limits_{i=1}^{m} L(y_{t_i}, \\langle w_t, x_{t_i}\\rangle) + \\gamma \\sum\\limits_{t=1}^T\\langle w_t, D^\\dagger w_t \\rangle\n","$$\n","\n","\\begin{align*}\n","min& \\qquad R(W, D) \\\\\n","s.t.& \\qquad W \\in \\mathbb{R}^{d\\times T} \\\\\n","& \\qquad D \\in \\mathbb{S}_+^d\\\\\n","& \\qquad trace(D) \\leq 1 \\\\\n","& \\qquad range(W) \\subseteq range(D)\n","\\end{align*}\n","That is, $(\\hat{A}, \\hat{U})$ is an optimal solution for our first formulation if and only if $(\\hat{W}, \\hat{D}) = (\\hat{U}\\hat{A}, \\hat{U}Diag(\\hat{\\lambda})\\hat{U}^T)$ is an optimal solution for the equivalent convex problem if and only if\n","$$\n","\\hat{\\lambda}_i := \\frac{||\\hat{a}^i||_2}{||\\hat{A}_{2,1}||}\n","$$\n","\n","### **Proof**.\n","Let $(W = UA)$ and $(D = U\\text{Diag}(\\frac{||a^i||_2}{||A||_{2,1}})U^T)$. Then $||a^i||_2 = ||W^Tu_i||_2$ and hence\n","\n","\\begin{align*}\n","\\sum\\limits_{t=1}^{T} \\langle w_t, D^\\dagger w_t \\rangle = \\mathrm{trace}(W^T D^\\dagger W) = ||A||_{2,1} \\mathrm{trace}(W^TU\\mathrm{Diag}(||W^Tu_i||_2)^\\dagger U^T W) = \\\\\n","||A||_{2, 1} \\mathrm{trace}(\\sum\\limits_{i=1}^d(||W^T u_i||_2)^\\dagger W^T u_iu_i^T W) = ||A||_{2,1}\\sum\\limits_{i=1}^d ||W^Tu_i||_2 = ||A||^2_{2,1}\n","\\end{align*}\n","Therefore, $( \\min_{W,D} R(W, D) \\leq \\min_{A,U} \\epsilon(A, U) )$. Conversely, let $(D = U\\text{Diag}(\\lambda)U^\\dagger$). Then\n","\n","\\begin{align*}\n","\\sum_{t=1}^{T} \\langle w_t, D^\\dagger w_t \\rangle &= \\text{trace}(W^\\dagger U\\text{Diag}(\\lambda+i)U^\\dagger W) \\\\\n","&= \\text{trace}\\left(\\text{Diag}(\\lambda+i)AA^\\dagger\\right) \\geq ||A||_{2,1}^2\n","\\end{align*}\n","\n","Where $\\epsilon(A, U) = \\sum\\limits_{t=1}^{T}\\sum\\limits_{i=1}^{m}L(y_{t_i}, \\langle a_t, U^T x_{t_i} \\rangle) + \\gamma ||A||^2_{2,1}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cvxpy as cp\n","import numpy as np"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"ename":"Exception","evalue":"At least one argument to quad_form must be non-variable.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn [142], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[0;32m     22\u001b[0m         objective \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mnorm2(Y[i, t] \u001b[38;5;241m-\u001b[39m cp\u001b[38;5;241m.\u001b[39msum(cp\u001b[38;5;241m.\u001b[39mmultiply(W[:, t], X[:, i, t])))\n\u001b[1;32m---> 23\u001b[0m     objective \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquad_form\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# objective += gamma * W[:, t].T  @ U @ W[:, t]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# objective += gamma * cp.sum(cp.multiply(W[:, t].T, W[:, t].T))\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# objective += cp.sum(cp.multiply(W[:, t].T, cp.matmul(U, W[:, t])))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#     for k in range(d):\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m#         objective += gamma * U[j, k] * W[j, t]* W[k, t]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m constraints \u001b[38;5;241m=\u001b[39m []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\cvxpy\\atoms\\quad_form.py:254\u001b[0m, in \u001b[0;36mquad_form\u001b[1;34m(x, P, assume_PSD)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuadForm(x, P)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one argument to quad_form must be non-variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m     )\n","\u001b[1;31mException\u001b[0m: At least one argument to quad_form must be non-variable."]}],"source":["d = 10\n","T = 5\n","m = 100\n","gamma = 0.1\n","\n","# Define the variables\n","coefficients = cp.Variable((d, T))\n","W = cp.Variable((d, T))\n","D = cp.Variable((d, d), symmetric=True)\n","D = cp.atoms.affine.wraps.psd_wrap(D)\n","U = cp.Variable((d, d), symmetric = True)\n","U = cp.atoms.affine.wraps.psd_wrap(U)\n","\n","X = np.random.rand(d, m, T)\n","Y = np.random.randint(2, size=(m, T))\n","\n","\n","objective = 0\n","tmp = []\n","for t in range(T):\n","    for i in range(m):\n","        objective += cp.norm2(Y[i, t] - cp.sum(cp.multiply(W[:, t], X[:, i, t])))\n","    objective += gamma * cp.quad_form(W[:, t], U)\n","    # objective += gamma * W[:, t].T  @ U @ W[:, t]\n","    # objective += gamma * cp.sum(cp.multiply(W[:, t].T, W[:, t].T))\n","    # objective += cp.sum(cp.multiply(W[:, t].T, cp.matmul(U, W[:, t])))\n","    # cp.sum(cp.multiply(x, y))\n","    # objective += gamma * cp.norm(W[:, t], 2)\n","    # for i in range(d):\n","    #     objective += W[i][t] * W[i][t]\n","    # for j in range(d):\n","    #     for k in range(d):\n","    #         objective += gamma * U[j, k] * W[j, t]* W[k, t]\n","    \n","constraints = []\n","constraints.append(D @ U == np.eye(d))\n","constraints.append(cp.trace(D) <= 1)\n","constraints.append(D >> 0)\n","constraints.append(U >> 0)\n","for t in range(T):\n","    constraints.append(W[:, t] == cp.sum([D[:, i] * coefficients[i, t] for i in range(d)]))\n","\n","problem = cp.Problem(objective=cp.Minimize(objective), constraints=constraints)\n","\n","problem.solve()\n","\n","# optimal_W = W.value\n","optimal_D = D.value"]},{"cell_type":"markdown","metadata":{},"source":["We tried different methods to implement $\\gamma \\sum\\limits_{t=1}^T\\langle w_t, D^\\dagger w_t \\rangle$ and some of the constraints, but we faced different kinds of errors in the process of multilying two cvx vector variables"]},{"cell_type":"markdown","metadata":{},"source":["## Relaxation\n","\n","when the matrix $U$ is not learned and we set $U = I_{d\\times d}$, our problem, computes a common set of variables across the tasks. That is, we have the following convex optimization problem\n","\n","\\begin{align*}\n","min \\qquad \\sum_{t=1}^T\\sum_{i=1}^m L(y_{t_i}, \\langle a_t, x_{t_i} \\rangle) + \\gamma ||A||_{2,1}^2\n","\\end{align*}"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[{"data":{"text/plain":["712.5946939520976"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["## Relaxtion\n","\n","d = 10\n","T = 5\n","m = 100 \n","gamma = 0.1\n","\n","A = cp.Variable((d, T))\n","\n","X = np.random.rand(d, m, T)\n","Y = np.random.randint(2, size=(m, T))\n","\n","objective = 0\n","for t in range(T):\n","    for i in range(m):\n","        objective += cp.norm2(Y[i, t] - cp.multiply(A[:, t], X[:, i, t]))\n","    objective += gamma * cp.norm2(A[:, t])**2\n","    \n","problem = cp.Problem(objective=cp.Minimize(objective))\n","\n","problem.solve()"]},{"cell_type":"markdown","metadata":{},"source":["## 3.3. Transferring knowledge of parameters\n","\n","In inductive transfer learning,\n","\n","$w_S = w_0 + v_S$\n","\n","$w_T = w_0 + v_T $\n","\n","where $ w_S$ and $ w_T $ are parameters of the SVMs for the source task and the target learning task, respectively. $ w_0 $ is a common parameter, while $ v_S $ and $ v_T $ are specific parameters for the source task and the target task, respectively. By assuming $ f_t = w_t \\cdot x $ to be a hyperplane for task $ t $, an extension of SVMs to the multitask learning case can be written as follows:\n","$f_t = w_t \\cdot x$\n","\n","\\begin{align*}\n","\\underset{w_0, v_t, \\xi_{t_i}}{min} & \\qquad J(w_0, v_t, \\xi_{t_i}) = \\sum\\limits_{t \\in \\{S, T\\}}\\sum\\limits_{i=1}^{n_t} \\xi_{t_i} + \\frac{\\lambda_1}{2} \\sum\\limits_{t \\in \\{S, T\\}} \\|v_t\\|^2 + \\lambda_2 \\|w_0\\|^2 \\\\\n","s.t. & \\qquad y_{t_i} (w_0 + v_t) x_{t_i} \\geq q - \\xi_{t_i},\\quad\\xi_{t_i} \\geq 0,\\; i \\in \\{1,2,\\cdots,n_t\\}\\;\\mathrm{and}\\;t \\in \\{S, T\\}\n","\\end{align*}"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"data":{"text/plain":["246.08815257879678"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["d = 10\n","T = 5\n","m = 100\n","lambda_1 = 0.1\n","lambda_2 = 0.1\n","\n","X = np.random.rand(d, m, T)\n","Y = np.random.randint(2, size=(m, T))\n","\n","xi = cp.Variable((T, m))\n","\n","w_0 = cp.Variable(d)\n","v = cp.Variable((T, d))\n","\n","objective = 0\n","for t in range(T):\n","    for i in range(m):\n","        objective += xi[t, i]\n","    objective += lambda_1 / 2 * cp.norm2(v[t, :]) ** 2\n","objective += lambda_2 * cp.norm2(w_0) ** 2\n","\n","constraints = []\n","for t in range(T):\n","    for i in range(m):\n","        constraints.append(xi[t, i] >= 0)\n","        constraints.append(Y[i, t] * (w_0 + v[t, :]) @ X[:,i, t] >= 1 - xi[t, i])\n","\n","problem = cp.Problem(objective=cp.Minimize(objective), constraints=constraints)\n","problem.solve()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
